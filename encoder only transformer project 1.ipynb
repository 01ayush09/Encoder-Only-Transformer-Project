{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eef4b9d",
   "metadata": {},
   "source": [
    "# Unified Encoder-Only Transformer NLP Project\n",
    "\n",
    "## **Objective**\n",
    "To build a **unified NLP system** using **Encoder-Only Transformer models** (like BERT, RoBERTa, DistilBERT) capable of:\n",
    "1. **Text Classification** – Assigning labels to entire documents or sentences.\n",
    "2. **Entity Extraction** – Identifying and classifying spans of text into categories (Named Entity Recognition, NER).\n",
    "3. **Semantic Similarity & Recommendations** – Measuring text similarity and finding the most relevant matches.\n",
    "\n",
    "---\n",
    "\n",
    "## **Purpose**\n",
    "- **Practical Demonstration**: Showcase how encoder-only transformer architectures can power multiple NLP tasks.\n",
    "- **Efficiency**: Consolidate code for training, evaluation, and inference across multiple NLP tasks.\n",
    "- **Flexibility**: Enable switching between datasets, models, and tasks using configuration files or minimal code edits.\n",
    "- **Reusability**: Serve as a foundation for chatbots, search engines, tagging systems, and recommendation platforms.\n",
    "\n",
    "---\n",
    "\n",
    "## **Workflow Overview**\n",
    "1. **Data Loading & Preprocessing**\n",
    "   - Load datasets for each task.\n",
    "   - Tokenize and prepare inputs for transformer models.\n",
    "2. **Model Selection**\n",
    "   - Use HuggingFace `transformers` library to load pre-trained encoder-only models.\n",
    "3. **Task-Specific Training**\n",
    "   - Text classification with `Trainer` API.\n",
    "   - Token classification (NER) with `Trainer` API and token alignment.\n",
    "   - Sentence similarity training with `sentence-transformers`.\n",
    "4. **Evaluation**\n",
    "   - Use accuracy, F1-score, precision, recall for classification/NER.\n",
    "   - Use cosine similarity scores and FAISS for recommendations.\n",
    "5. **Inference & Recommendations**\n",
    "   - Run predictions on new text.\n",
    "   - Retrieve similar documents/sentences from FAISS index.\n",
    "\n",
    "---\n",
    "\n",
    "## **Expected Inputs**\n",
    "- **Text Classification**: CSV with `text` and `label` columns.\n",
    "- **Entity Extraction (NER)**: Dataset in `tokens` and `ner_tags` format.\n",
    "- **Similarity**: Sentences/documents in plain text or CSV.\n",
    "\n",
    "## **Expected Outputs**\n",
    "- **Classification**: Predicted label for each text.\n",
    "- **Entity Extraction**: List of entities with positions and types.\n",
    "- **Similarity**: Ranked list of similar texts with similarity scores.\n",
    "\n",
    "---\n",
    "\n",
    "## **Applications**\n",
    "- Chatbots with intent detection and entity recognition.\n",
    "- Automated tagging of news articles, emails, or documents.\n",
    "- Semantic search and recommendation engines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y5QWBN3ZF3O3"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Unified project combining three notebooks:\n",
    "- Classification + Multi-label Tagging\n",
    "- Named Entity Recognition (Token Classification)\n",
    "- Semantic Similarity & Recommendation (Sentence Transformers + FAISS)\n",
    "\n",
    "File: encoder_only_transformer_unified_project.py\n",
    "Author: Generated for user\n",
    "\n",
    "Usage examples (from terminal):\n",
    "    # Classification training\n",
    "    python encoder_only_transformer_unified_project.py --task classification --config configs/classification.yaml\n",
    "\n",
    "    # NER training\n",
    "    python encoder_only_transformer_unified_project.py --task ner --config configs/ner.yaml\n",
    "\n",
    "    # Similarity training / index build\n",
    "    python encoder_only_transformer_unified_project.py --task similarity --config configs/similarity.yaml\n",
    "\n",
    "This single-file project organizes data loading, model building, training and evaluation helpers.\n",
    "Replace dataset paths and small placeholders with your real data.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import yaml\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "# Core ML libs\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "\n",
    "# For similarity\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses, evaluation\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Utilities\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Config dataclasses (simple)\n",
    "# -----------------------------\n",
    "@dataclass\n",
    "class CommonConfig:\n",
    "    model_name: str = \"bert-base-uncased\"\n",
    "    output_dir: str = \"outputs\"\n",
    "    max_length: int = 256\n",
    "    batch_size: int = 16\n",
    "    num_epochs: int = 3\n",
    "    seed: int = 42\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Data modules (placeholders)\n",
    "# -----------------------------\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts: List[str], labels: List[int], tokenizer, max_length=256):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoding = self.tokenizer(text, truncation=True, padding=False, max_length=self.max_length)\n",
    "        item = {k: torch.tensor(v) for k, v in encoding.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "\n",
    "class TokenClassificationDataset(Dataset):\n",
    "    \"\"\"Simple token classification dataset where inputs are pre-tokenized using the same tokenizer.\n",
    "    Expects a list of dicts: {\"tokens\": [...], \"labels\": [...]} where labels are ints aligned to tokens.\n",
    "    \"\"\"\n",
    "    def __init__(self, examples: List[Dict], tokenizer, label2id: Dict[str, int], max_length=256):\n",
    "        self.examples = examples\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label2id = label2id\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ex = self.examples[idx]\n",
    "        tokens = ex['tokens']\n",
    "        labels = ex['labels']\n",
    "        # Use tokenizer.encode_plus with is_split_into_words for alignment\n",
    "        encoding = self.tokenizer(tokens,\n",
    "                                  is_split_into_words=True,\n",
    "                                  truncation=True,\n",
    "                                  padding=False,\n",
    "                                  max_length=self.max_length)\n",
    "        word_ids = encoding.word_ids()\n",
    "        label_ids = []\n",
    "        prev_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != prev_word_idx:\n",
    "                # assign the label for the first token of the word\n",
    "                label_ids.append(labels[word_idx])\n",
    "            else:\n",
    "                # for subsequent tokens of a word, you can set -100 or label depending on your choice\n",
    "                label_ids.append(-100)\n",
    "            prev_word_idx = word_idx\n",
    "\n",
    "        item = {k: torch.tensor(v) for k, v in encoding.items()}\n",
    "        item['labels'] = torch.tensor(label_ids, dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Model builders\n",
    "# -----------------------------\n",
    "class MultiTaskEncoder(nn.Module):\n",
    "    \"\"\"Encoder-only model with two heads: classification (single-label) and multi-label tagging (sigmoid outputs)\n",
    "    This wraps a pretrained encoder and two heads.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str, num_classes: int, num_tags: int):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        hidden_size = self.encoder.config.hidden_size\n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "        self.multitag_head = nn.Linear(hidden_size, num_tags)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        pooled = outputs.last_hidden_state[:, 0, :]  # use [CLS] token\n",
    "        class_logits = self.classifier(pooled)\n",
    "        multitag_logits = self.multitag_head(pooled)\n",
    "        return {\n",
    "            'class_logits': class_logits,\n",
    "            'multitag_logits': multitag_logits,\n",
    "        }\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Training helpers\n",
    "# -----------------------------\n",
    "\n",
    "def train_classification_with_trainer(train_df: pd.DataFrame, val_df: pd.DataFrame, config: CommonConfig, label_list: List[str]):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "    # create datasets\n",
    "    train_texts, train_labels = train_df['text'].tolist(), train_df['label'].tolist()\n",
    "    val_texts, val_labels = val_df['text'].tolist(), val_df['label'].tolist()\n",
    "\n",
    "    train_dataset = TextDataset(train_texts, train_labels, tokenizer, max_length=config.max_length)\n",
    "    val_dataset = TextDataset(val_texts, val_labels, tokenizer, max_length=config.max_length)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(config.model_name, num_labels=len(label_list))\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=config.output_dir,\n",
    "        per_device_train_batch_size=config.batch_size,\n",
    "        per_device_eval_batch_size=config.batch_size,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        num_train_epochs=config.num_epochs,\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "    def compute_metrics(pred):\n",
    "        labels = pred.label_ids\n",
    "        preds = np.argmax(pred.predictions, axis=1)\n",
    "        return {\"f1\": f1_score(labels, preds, average='weighted')}\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model(config.output_dir)\n",
    "\n",
    "\n",
    "def train_token_classification_with_trainer(train_examples: List[Dict], val_examples: List[Dict], config: CommonConfig, label_list: List[str]):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "    label2id = {l: i for i, l in enumerate(label_list)}\n",
    "\n",
    "    train_dataset = TokenClassificationDataset(train_examples, tokenizer, label2id, max_length=config.max_length)\n",
    "    val_dataset = TokenClassificationDataset(val_examples, tokenizer, label2id, max_length=config.max_length)\n",
    "\n",
    "    model = AutoModelForTokenClassification.from_pretrained(config.model_name, num_labels=len(label_list))\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=config.output_dir,\n",
    "        per_device_train_batch_size=config.batch_size,\n",
    "        per_device_eval_batch_size=config.batch_size,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        num_train_epochs=config.num_epochs,\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "    # Note: compute_metrics for token classification needs alignment handling; keep simple here\n",
    "    def compute_metrics(pred):\n",
    "        # pred.predictions shape = (batch, seq_len, num_labels)\n",
    "        preds = np.argmax(pred.predictions, axis=-1).flatten()\n",
    "        labels = pred.label_ids.flatten()\n",
    "        mask = labels != -100\n",
    "        return {\"f1\": f1_score(labels[mask], preds[mask], average='weighted')}\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model(config.output_dir)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Similarity training and FAISS index\n",
    "# -----------------------------\n",
    "\n",
    "def train_similarity_sentence_transformer(train_pairs: List[Tuple[str, str, int]], model_name: str, output_dir: str, epochs=1, batch_size=8):\n",
    "    # train_pairs is list of (sent1, sent2, label) where label 1=similar, 0=not similar\n",
    "    sents_train = [InputExample(texts=[a, b], label=float(label)) for a, b, label in train_pairs]\n",
    "    model = SentenceTransformer(model_name)\n",
    "    train_loader = torch.utils.data.DataLoader(sents_train, shuffle=True, batch_size=batch_size)\n",
    "    loss = losses.CosineSimilarityLoss(model)\n",
    "    model.fit(train_objectives=[(train_loader, loss)], epochs=epochs, output_path=output_dir)\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_faiss_index(sentences: List[str], model: SentenceTransformer, index_path: str, dim: Optional[int] = None):\n",
    "    embeddings = model.encode(sentences, convert_to_numpy=True, show_progress_bar=True)\n",
    "    if dim is None:\n",
    "        dim = embeddings.shape[1]\n",
    "\n",
    "    index = faiss.IndexFlatIP(dim)  # inner product for cosine if normalized\n",
    "    # normalize for cosine\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    index.add(embeddings)\n",
    "    faiss.write_index(index, index_path)\n",
    "    # Save sentences mapping\n",
    "    with open(index_path + '.meta', 'w', encoding='utf-8') as f:\n",
    "        for s in sentences:\n",
    "            f.write(s.replace('\\n', ' ') + '\\n')\n",
    "    return index\n",
    "\n",
    "\n",
    "def query_faiss(index_path: str, model: SentenceTransformer, query: str, top_k=5):\n",
    "    index = faiss.read_index(index_path)\n",
    "    with open(index_path + '.meta', 'r', encoding='utf-8') as f:\n",
    "        sentences = [line.strip() for line in f]\n",
    "    q_emb = model.encode([query], convert_to_numpy=True)\n",
    "    faiss.normalize_L2(q_emb)\n",
    "    D, I = index.search(q_emb, top_k)\n",
    "    return [(sentences[i], float(D[0][k])) for k, i in enumerate(I[0])]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# CLI and main orchestration\n",
    "# -----------------------------\n",
    "\n",
    "def parse_args():\n",
    "    p = argparse.ArgumentParser()\n",
    "    p.add_argument('--task', type=str, choices=['classification', 'ner', 'similarity'], required=True)\n",
    "    p.add_argument('--config', type=str, required=True, help='Path to yaml config for the task')\n",
    "    return p.parse_args()\n",
    "\n",
    "\n",
    "def load_config(path: str) -> dict:\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    cfg = load_config(args.config)\n",
    "\n",
    "    common = CommonConfig(**cfg.get('common', {}))\n",
    "\n",
    "    if args.task == 'classification':\n",
    "        # Expect train/val csv with columns text,label\n",
    "        train_df = pd.read_csv(cfg['train_csv'])\n",
    "        val_df = pd.read_csv(cfg['val_csv'])\n",
    "        label_list = cfg['label_list']\n",
    "        common.model_name = cfg.get('model_name', common.model_name)\n",
    "        common.output_dir = cfg.get('output_dir', common.output_dir)\n",
    "        common.max_length = cfg.get('max_length', common.max_length)\n",
    "        common.batch_size = cfg.get('batch_size', common.batch_size)\n",
    "        common.num_epochs = cfg.get('num_epochs', common.num_epochs)\n",
    "        train_classification_with_trainer(train_df, val_df, common, label_list)\n",
    "\n",
    "    elif args.task == 'ner':\n",
    "        # Expect jsonl or python list of examples with tokens + labels\n",
    "        # Example input format: a json file with [{'tokens': [...], 'labels': [...]}, ...]\n",
    "        import json\n",
    "        with open(cfg['train_json'], 'r', encoding='utf-8') as f:\n",
    "            train_examples = json.load(f)\n",
    "        with open(cfg['val_json'], 'r', encoding='utf-8') as f:\n",
    "            val_examples = json.load(f)\n",
    "        label_list = cfg['label_list']\n",
    "        common.model_name = cfg.get('model_name', common.model_name)\n",
    "        common.output_dir = cfg.get('output_dir', common.output_dir)\n",
    "        common.max_length = cfg.get('max_length', common.max_length)\n",
    "        common.batch_size = cfg.get('batch_size', common.batch_size)\n",
    "        common.num_epochs = cfg.get('num_epochs', common.num_epochs)\n",
    "        train_token_classification_with_trainer(train_examples, val_examples, common, label_list)\n",
    "\n",
    "    elif args.task == 'similarity':\n",
    "        # Expect a csv or json with pairs for training and a corpus file for indexing\n",
    "        train_pairs = []\n",
    "        if 'train_pairs_csv' in cfg:\n",
    "            df = pd.read_csv(cfg['train_pairs_csv'])\n",
    "            # expect columns a,b,label\n",
    "            train_pairs = list(df[['a','b','label']].itertuples(index=False, name=None))\n",
    "        model_name = cfg.get('model_name', common.model_name)\n",
    "        output_dir = cfg.get('output_dir', common.output_dir)\n",
    "        epochs = cfg.get('num_epochs', 1)\n",
    "        model = train_similarity_sentence_transformer(train_pairs, model_name, output_dir, epochs=epochs, batch_size=cfg.get('batch_size',8))\n",
    "        # build index if corpus provided\n",
    "        if 'corpus_txt' in cfg:\n",
    "            with open(cfg['corpus_txt'], 'r', encoding='utf-8') as f:\n",
    "                sentences = [l.strip() for l in f if l.strip()]\n",
    "            index_path = os.path.join(output_dir, 'faiss.index')\n",
    "            build_faiss_index(sentences, model, index_path)\n",
    "            print('FAISS index written to', index_path)\n",
    "\n",
    "    else:\n",
    "        raise ValueError('Unknown task')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
