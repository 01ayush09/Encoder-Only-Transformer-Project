{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dfb7587",
   "metadata": {},
   "source": [
    "# WhatsApp Message Importance Recommendation System\n",
    "This project modifies a BERT-style encoder-only Transformer to classify WhatsApp-like messages as **Important** or **Not Important** using a synthetic dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0e5fa8",
   "metadata": {},
   "source": [
    "## Step 1: Install & Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863fab71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install transformers torch scikit-learn pandas\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import BertTokenizer, BertModel\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1817dbf1",
   "metadata": {},
   "source": [
    "## Step 2: Load Synthetic WhatsApp Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189cc01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"whatsapp_messages.csv\")\n",
    "print(df.head())\n",
    "\n",
    "# Train-test split\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df[\"message\"].values, df[\"label\"].values, test_size=0.2, random_state=42\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2306eceb",
   "metadata": {},
   "source": [
    "## Step 3: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8769c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True, return_tensors=\"pt\")\n",
    "test_encodings = tokenizer(list(test_texts), truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# Convert labels to integers\n",
    "label_map = {\"not_important\": 0, \"important\": 1}\n",
    "train_labels = torch.tensor([label_map[label] for label in train_labels])\n",
    "test_labels = torch.tensor([label_map[label] for label in test_labels])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c3aaa8",
   "metadata": {},
   "source": [
    "## Step 4: Define Transformer-based Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66714868",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ImportanceClassifier(nn.Module):\n",
    "    def __init__(self, hidden_size=768, num_labels=2):\n",
    "        super(ImportanceClassifier, self).__init__()\n",
    "        self.encoder = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "        cls_output = self.dropout(cls_output)\n",
    "        logits = self.classifier(cls_output)\n",
    "        return logits\n",
    "\n",
    "model = ImportanceClassifier()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c524b2a6",
   "metadata": {},
   "source": [
    "## Step 5: Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b82e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = TensorDataset(train_encodings[\"input_ids\"], train_encodings[\"attention_mask\"], train_labels)\n",
    "test_dataset = TensorDataset(test_encodings[\"input_ids\"], test_encodings[\"attention_mask\"], test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f53edd",
   "metadata": {},
   "source": [
    "## Step 6: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c020a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epochs = 2\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdf314f",
   "metadata": {},
   "source": [
    "## Step 7: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e84b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "for batch in test_dataset:\n",
    "    input_ids, attention_mask, labels = [b.unsqueeze(0).to(device) for b in batch]\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "    preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "    all_preds.extend(preds)\n",
    "    all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "print(classification_report(all_labels, all_preds, target_names=[\"not_important\", \"important\"]))\n",
    "    "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
