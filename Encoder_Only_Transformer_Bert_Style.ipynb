{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUGLWvs-MLAf"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Encoder-only Transformer (BERT-style) â€” single-file project\n",
        "Features added:\n",
        "- Transformer Encoder implementation (PyTorch)\n",
        "- TokenEmbedding + SegmentEmbedding + PositionEmbedding\n",
        "- Masked Language Modeling (MLM) head for pre-training\n",
        "- Classification head (using [CLS])\n",
        "- Question-Answering heads (start & end logits)\n",
        "- Utilities to freeze encoder (feature extraction), domain-adaptation, and simple distillation stub\n",
        "- Training loop placeholders and example usage\n",
        "\n",
        "Notes for the user:\n",
        "- Replace the dataset placeholders with your dataset loader/tokenizer.\n",
        "- This file is intentionally framework-agnostic for tokenization (assumes a tokenizer that maps text -> input_ids, attention_mask, token_type_ids).\n",
        "- If you prefer TensorFlow, tell me and I'll convert it.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "# -----------------------------\n",
        "# Basic Modules\n",
        "# -----------------------------\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, hidden_size: int):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, hidden_size)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        return self.emb(input_ids)\n",
        "\n",
        "class PositionEmbedding(nn.Module):\n",
        "    def __init__(self, max_len: int, hidden_size: int):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(max_len, hidden_size)\n",
        "\n",
        "    def forward(self, seq_len):\n",
        "        # seq_len may be an int or tensor shape\n",
        "        if isinstance(seq_len, int):\n",
        "            pos_ids = torch.arange(seq_len, device=self.emb.weight.device).unsqueeze(0)\n",
        "        else:\n",
        "            pos_ids = torch.arange(seq_len.size(1), device=self.emb.weight.device).unsqueeze(0)\n",
        "        return self.emb(pos_ids)\n",
        "\n",
        "class SegmentEmbedding(nn.Module):\n",
        "    def __init__(self, type_vocab_size: int, hidden_size: int):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(type_vocab_size, hidden_size)\n",
        "\n",
        "    def forward(self, token_type_ids):\n",
        "        return self.emb(token_type_ids)\n",
        "\n",
        "# -----------------------------\n",
        "# Transformer Encoder Block\n",
        "# -----------------------------\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, hidden_size, num_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert hidden_size % num_heads == 0\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = hidden_size // num_heads\n",
        "        self.qkv = nn.Linear(hidden_size, 3 * hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask: Optional[torch.Tensor] = None):\n",
        "        B, T, H = x.size()\n",
        "        qkv = self.qkv(x)  # (B, T, 3H)\n",
        "        qkv = qkv.reshape(B, T, 3, self.num_heads, self.head_dim)\n",
        "        q, k, v = qkv.unbind(dim=2)\n",
        "        # transpose for attention: (B, heads, T, head_dim)\n",
        "        q = q.permute(0, 2, 1, 3)\n",
        "        k = k.permute(0, 2, 1, 3)\n",
        "        v = v.permute(0, 2, 1, 3)\n",
        "        # scaled dot-product\n",
        "        scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "        if mask is not None:\n",
        "            # mask: (B, 1, 1, T) or broadcastable\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "        out = attn @ v  # (B, heads, T, head_dim)\n",
        "        out = out.permute(0, 2, 1, 3).contiguous().reshape(B, T, H)\n",
        "        return self.out(out)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, hidden_size, intermediate_size, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(hidden_size, intermediate_size)\n",
        "        self.act = nn.GELU()\n",
        "        self.fc2 = nn.Linear(intermediate_size, hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.dropout(self.fc2(self.act(self.fc1(x))))\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, hidden_size, num_heads, intermediate_size, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadSelfAttention(hidden_size, num_heads, dropout)\n",
        "        self.ln1 = nn.LayerNorm(hidden_size)\n",
        "        self.ff = FeedForward(hidden_size, intermediate_size, dropout)\n",
        "        self.ln2 = nn.LayerNorm(hidden_size)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        attn_out = self.attn(self.ln1(x), mask)\n",
        "        x = x + attn_out\n",
        "        ff_out = self.ff(self.ln2(x))\n",
        "        x = x + ff_out\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, num_layers, hidden_size, num_heads, intermediate_size, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            EncoderLayer(hidden_size, num_heads, intermediate_size, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.ln_final = nn.LayerNorm(hidden_size)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.ln_final(x)\n",
        "\n",
        "# -----------------------------\n",
        "# BERT-style Model\n",
        "# -----------------------------\n",
        "class BertStyleModel(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size: int = 30522,\n",
        "                 hidden_size: int = 768,\n",
        "                 max_position_embeddings: int = 512,\n",
        "                 type_vocab_size: int = 2,\n",
        "                 num_layers: int = 12,\n",
        "                 num_heads: int = 12,\n",
        "                 intermediate_size: int = 3072,\n",
        "                 dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.token_emb = TokenEmbedding(vocab_size, hidden_size)\n",
        "        self.pos_emb = PositionEmbedding(max_position_embeddings, hidden_size)\n",
        "        self.segment_emb = SegmentEmbedding(type_vocab_size, hidden_size)\n",
        "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.encoder = Encoder(num_layers, hidden_size, num_heads, intermediate_size, dropout)\n",
        "        # Heads\n",
        "        self.mlm_head = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.GELU(),\n",
        "            nn.LayerNorm(hidden_size),\n",
        "            nn.Linear(hidden_size, vocab_size)\n",
        "        )\n",
        "        self.classifier = nn.Linear(hidden_size, 2)  # default binary\n",
        "        self.qa_start = nn.Linear(hidden_size, 1)\n",
        "        self.qa_end = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward_embeddings(self, input_ids, token_type_ids=None):\n",
        "        # input_ids: (B, T)\n",
        "        B, T = input_ids.size()\n",
        "        tok = self.token_emb(input_ids)\n",
        "        pos = self.pos_emb(T)\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = torch.zeros_like(input_ids)\n",
        "        seg = self.segment_emb(token_type_ids)\n",
        "        embeddings = tok + pos + seg\n",
        "        embeddings = self.layer_norm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
        "        embeddings = self.forward_embeddings(input_ids, token_type_ids)\n",
        "        # attention_mask: (B, T) where 1=keep 0=pad\n",
        "        if attention_mask is not None:\n",
        "            # convert to shape (B, 1, 1, T) for broadcasting\n",
        "            mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "        else:\n",
        "            mask = None\n",
        "        encoded = self.encoder(embeddings, mask)\n",
        "        return encoded\n",
        "\n",
        "    # ---------- Task-specific helpers ----------\n",
        "    def mlm(self, input_ids, mask_positions, token_type_ids=None):\n",
        "        \"\"\"\n",
        "        input_ids: (B, T)\n",
        "        mask_positions: bool tensor (B, T) indicating where [MASK] tokens are\n",
        "        returns logits for vocab at masked positions\n",
        "        \"\"\"\n",
        "        encoded = self.forward(input_ids, attention_mask=(input_ids!=0).long(), token_type_ids=token_type_ids)\n",
        "        logits = self.mlm_head(encoded)  # (B, T, V)\n",
        "        # Optionally gather masked positions outside\n",
        "        return logits\n",
        "\n",
        "    def classify(self, input_ids, attention_mask=None, token_type_ids=None, num_labels=2):\n",
        "        encoded = self.forward(input_ids, attention_mask, token_type_ids)\n",
        "        cls_embed = encoded[:, 0, :]  # assume [CLS] token at position 0\n",
        "        logits = self.classifier(cls_embed)\n",
        "        if num_labels == 1:\n",
        "            return logits.squeeze(-1)\n",
        "        return logits\n",
        "\n",
        "    def qa(self, input_ids, attention_mask=None, token_type_ids=None):\n",
        "        encoded = self.forward(input_ids, attention_mask, token_type_ids)\n",
        "        start_logits = self.qa_start(encoded).squeeze(-1)  # (B, T)\n",
        "        end_logits = self.qa_end(encoded).squeeze(-1)\n",
        "        return start_logits, end_logits\n",
        "\n",
        "    # Utility: freeze encoder (feature extraction mode)\n",
        "    def freeze_encoder(self):\n",
        "        for p in self.encoder.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    def unfreeze_encoder(self):\n",
        "        for p in self.encoder.parameters():\n",
        "            p.requires_grad = True\n",
        "\n",
        "# -----------------------------\n",
        "# Training Utilities (skeleton)\n",
        "# -----------------------------\n",
        "\n",
        "def compute_mlm_loss(logits, target_ids, mask_positions):\n",
        "    # logits: (B, T, V), target_ids: (B, T), mask_positions: (B, T) bool\n",
        "    vocab_size = logits.size(-1)\n",
        "    logits_flat = logits.view(-1, vocab_size)\n",
        "    targets_flat = target_ids.view(-1)\n",
        "    mask_flat = mask_positions.view(-1)\n",
        "    if mask_flat.sum() == 0:\n",
        "        return torch.tensor(0.0, device=logits.device)\n",
        "    loss_fct = nn.CrossEntropyLoss(reduction='none')\n",
        "    losses = loss_fct(logits_flat, targets_flat)\n",
        "    masked_losses = losses * mask_flat.float()\n",
        "    return masked_losses.sum() / mask_flat.sum().float()\n",
        "\n",
        "\n",
        "def compute_classification_loss(logits, labels):\n",
        "    if logits.size(-1) == 1:\n",
        "        # regression / single-label\n",
        "        loss = F.mse_loss(logits.squeeze(-1), labels.float())\n",
        "    else:\n",
        "        loss = F.cross_entropy(logits, labels.long())\n",
        "    return loss\n",
        "\n",
        "\n",
        "def compute_qa_loss(start_logits, end_logits, start_positions, end_positions):\n",
        "    loss_fct = nn.CrossEntropyLoss()\n",
        "    start_loss = loss_fct(start_logits, start_positions)\n",
        "    end_loss = loss_fct(end_logits, end_positions)\n",
        "    return (start_loss + end_loss) / 2\n",
        "\n",
        "# Simple training step skeleton (single-batch)\n",
        "def train_step_mlm(model: BertStyleModel, batch, optimizer):\n",
        "    model.train()\n",
        "    input_ids = batch['input_ids']\n",
        "    token_type_ids = batch.get('token_type_ids')\n",
        "    attention_mask = batch.get('attention_mask')\n",
        "    mask_positions = batch['mask_positions']  # bool\n",
        "    mlm_labels = batch['mlm_labels']\n",
        "\n",
        "    logits = model.mlm(input_ids, mask_positions, token_type_ids)\n",
        "    loss = compute_mlm_loss(logits, mlm_labels, mask_positions)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "# Fine-tuning step for classification\n",
        "def train_step_classification(model: BertStyleModel, batch, optimizer):\n",
        "    model.train()\n",
        "    input_ids = batch['input_ids']\n",
        "    token_type_ids = batch.get('token_type_ids')\n",
        "    attention_mask = batch.get('attention_mask')\n",
        "    labels = batch['labels']\n",
        "\n",
        "    logits = model.classify(input_ids, attention_mask, token_type_ids)\n",
        "    loss = compute_classification_loss(logits, labels)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "# Fine-tuning step for QA\n",
        "def train_step_qa(model: BertStyleModel, batch, optimizer):\n",
        "    model.train()\n",
        "    input_ids = batch['input_ids']\n",
        "    token_type_ids = batch.get('token_type_ids')\n",
        "    attention_mask = batch.get('attention_mask')\n",
        "    start_positions = batch['start_positions']\n",
        "    end_positions = batch['end_positions']\n",
        "\n",
        "    start_logits, end_logits = model.qa(input_ids, attention_mask, token_type_ids)\n",
        "    loss = compute_qa_loss(start_logits, end_logits, start_positions, end_positions)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "# -----------------------------\n",
        "# Distillation stub (teacher -> student)\n",
        "# -----------------------------\n",
        "def distill_step(teacher: BertStyleModel, student: BertStyleModel, batch, optimizer, alpha=0.5, temperature=2.0):\n",
        "    \"\"\"\n",
        "    Simple distillation recipe: match logits of masked LM or [CLS] distribution.\n",
        "    This is a stub â€” adapt per your target (MLM distillation or task distillation).\n",
        "    \"\"\"\n",
        "    teacher.eval()\n",
        "    student.train()\n",
        "    input_ids = batch['input_ids']\n",
        "    token_type_ids = batch.get('token_type_ids')\n",
        "    attention_mask = batch.get('attention_mask')\n",
        "    mask_positions = batch['mask_positions']\n",
        "    mlm_labels = batch['mlm_labels']\n",
        "\n",
        "    with torch.no_grad():\n",
        "        t_logits = teacher.mlm(input_ids, mask_positions, token_type_ids)\n",
        "    s_logits = student.mlm(input_ids, mask_positions, token_type_ids)\n",
        "    # focus only on masked tokens\n",
        "    vocab = s_logits.size(-1)\n",
        "    s_flat = s_logits.view(-1, vocab)\n",
        "    t_flat = t_logits.view(-1, vocab)\n",
        "    mask_flat = mask_positions.view(-1)\n",
        "    if mask_flat.sum() == 0:\n",
        "        return None\n",
        "    # soft targets loss\n",
        "    t_soft = F.log_softmax(t_flat / temperature, dim=-1)\n",
        "    s_soft = F.log_softmax(s_flat / temperature, dim=-1)\n",
        "    kd_loss = F.kl_div(s_soft, t_soft.exp(), reduction='none').sum(dim=1)\n",
        "    kd_loss = (kd_loss * mask_flat.float()).sum() / mask_flat.sum().float()\n",
        "    # hard mlm loss for student\n",
        "    mlm_loss = compute_mlm_loss(s_logits, mlm_labels, mask_positions)\n",
        "    loss = alpha * kd_loss + (1 - alpha) * mlm_loss\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "# -----------------------------\n",
        "# Example usage (pseudocode)\n",
        "# -----------------------------\n",
        "if __name__ == '__main__':\n",
        "    # Hyperparameters (tune for your environment)\n",
        "    VOCAB_SIZE = 30522\n",
        "    model = BertStyleModel(vocab_size=VOCAB_SIZE,\n",
        "                           hidden_size=256,     # smaller for quick runs\n",
        "                           max_position_embeddings=512,\n",
        "                           num_layers=4,\n",
        "                           num_heads=8,\n",
        "                           intermediate_size=1024)\n",
        "\n",
        "    # Example: prepare a batch (replace with real tokenizer/dataloader)\n",
        "    # Assume tokenizer returns input_ids, token_type_ids, attention_mask\n",
        "    # And for MLM: mask_positions boolean mask and mlm_labels with target ids at masked positions\n",
        "    dummy_batch = {\n",
        "        'input_ids': torch.randint(0, VOCAB_SIZE, (2, 64)),\n",
        "        'token_type_ids': torch.zeros((2, 64), dtype=torch.long),\n",
        "        'attention_mask': torch.ones((2, 64), dtype=torch.long),\n",
        "        'mask_positions': torch.zeros((2, 64), dtype=torch.bool),\n",
        "        'mlm_labels': torch.zeros((2, 64), dtype=torch.long),\n",
        "        'labels': torch.tensor([0, 1]),\n",
        "        'start_positions': torch.tensor([10, 5]),\n",
        "        'end_positions': torch.tensor([12, 7])\n",
        "    }\n",
        "\n",
        "    optim = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "    # Train a single step (demo)\n",
        "    loss = train_step_classification(model, dummy_batch, optim)\n",
        "    print('demo loss (classification):', loss)\n",
        "\n",
        "    # Freeze encoder for feature-extraction\n",
        "    model.freeze_encoder()\n",
        "    # ...train only heads\n",
        "\n",
        "    # To resume full fine-tuning\n",
        "    model.unfreeze_encoder()\n",
        "\n",
        "    print('Model ready. Replace dummy_batch with your dataloader and tokenizer.')\n",
        "\n",
        "# End of file\n"
      ]
    }
  ]
}